{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Arg Max and Soft Max\n",
        "---\n",
        "\n",
        "Prerequisite:\n",
        "- working and main idea behind neural nets\n",
        "- main idea behind backpropagation\n",
        "- How neural networks works with multiple input and output nodes.\n",
        "\n",
        "\n",
        "## Why Arg-Max or Soft-Max?\n",
        "\n",
        "In multi input output neural network we have seen that when we plug two input values to the network for sepal width and petal width, the network will give us the three o/p values for setosa, versicolor, and verginica.\n",
        "\n",
        "And we can notice that it does not gives us output in range of 0 to 1.\n",
        "\n",
        "like some times it will give us -1.7, or 0.6, or 1.8 something like that for multiple output nodes.\n",
        "\n",
        "**Reason one**\n",
        "And  these **broad range of raw output makes it harder to interpret** the results.\n",
        "\n",
        "\n",
        "That's why before the final decision is made, we pass raw output to the either Arg-max or Soft-max\n"
      ],
      "metadata": {
        "id": "OSp7cpApM193"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## **Arg-max**\n",
        "\n",
        "![](https://raw.githubusercontent.com/srkds/deep-learning-foundation/main/assets/argmax.jpg)\n",
        "\n",
        "Simply sets 1 to the larger value of the all raw output values and sets others to 0.\n",
        "\n",
        "**For example**\n",
        "\n",
        "from below table we can observe that Argmax sets final output of setosa to 1. and others to 0\n",
        "\n",
        "\n",
        "|Label|Raw output| Argmax value |\n",
        "|---|---|---|\n",
        "|Setosa|1.43|1|\n",
        "|Versicolor|-0.4|0|\n",
        "|Virginica|0.23|0|\n",
        "\n",
        "**Takeaway**\n",
        "\n",
        "so when we use Argmax in NN, the prediction is simply the output with a 1 in it.\n",
        "\n",
        "And it makes output super easy to interpret.\n",
        "\n",
        "**Problem with Argmax**\n",
        "\n",
        "We can't use it to optimize the weights and biases in the neural networks\n",
        "\n",
        "This is because the output values from Argmax are constant/discrete 0 and 1.\n",
        "\n",
        "Understand it by below shown diagram.\n",
        "![](https://raw.githubusercontent.com/srkds/deep-learning-foundation/main/assets/argmax-derivative.jpg)\n",
        "\n",
        "Ploting the second largest output value, 0.23 on a graph.\n",
        "\n",
        "Since argmax will give 1 for any output value  grater than 0.23 and it will output 0 for any output value less than 0.23.\n",
        "\n",
        "Because the slope of the two lines are both 0, their derivative are also zero.\n",
        "\n",
        "That means when we want to find optimal value of any weights or biases of the network then we will endup plugin 0  into the chain rule as the derivative of Argmax wrt some raw output values. Then the whole derivative will be 0.\n",
        "\n",
        "And if we plug 0 into gradient decent then we won't step towards the optimal parameter values.\n",
        "\n",
        "**ðŸ“Œ That means we can't use ArgMax for backpropagation.**\n",
        "\n",
        "\n",
        "$$ \\frac{d \\; Loss}{some_weight} = \\frac{d \\; Loss}{d \\;ArgMax } * \\frac{d \\; ArgMax }{d \\; Raw output} * ... $$\n",
        "\n",
        "\n",
        "$$ \\frac{d \\; Loss}{some_weight} = \\frac{d \\; Loss}{d \\;ArgMax } * 0 * ... $$\n",
        "\n",
        "\n",
        "$$ \\frac{d \\; Loss}{some_weight} = 0 $$\n",
        "\n",
        "\n",
        "## SoftMax function\n",
        "\n",
        "![](https://raw.githubusercontent.com/srkds/deep-learning-foundation/main/assets/softmax.jpg)\n",
        "\n",
        "So problem of ArgMax leads us to move on SoftMax function.\n",
        "\n",
        "When we want to use ArgMax for output we can use SoftMax for training.\n",
        "\n",
        "Let's do some calculation.\n",
        "\n",
        "**Softmax output value for Setosa**\n",
        "\n",
        "$$  SoftMax_{setosa}(Output Values) =  \\frac{e^{setosa}}{e^{setosa} * e^{versicolor} * e^{virginica} } $$\n",
        "\n",
        "$$  SoftMax_{setosa}(Output Values) =  \\frac{e^{1.43}}{e^{1.43} * e^{-0.4} * e^{0.23} } = 0.69 $$\n",
        "\n",
        "0.69 Softmax output value for setosa.\n",
        "\n",
        "**Softmax o/p value for Versicolor**\n",
        "\n",
        "Only the changes at numerator\n",
        "$$  SoftMax_{versicolor}(Output Values) =  \\frac{e^{versicolor}}{e^{setosa} * e^{versicolor} * e^{virginica} } $$\n",
        "\n",
        "$$  SoftMax_{versicolor}(Output Values) =  \\frac{e^{-0.4}}{e^{1.43} * e^{-0.4} * e^{0.23} } = 0.10 $$\n",
        "\n",
        "0.10 Softmax output value for versicolor.\n",
        "\n",
        "\n",
        "**Finally the Softmax o/p value for Versicolor**\n",
        "\n",
        "Only the changes at numerator\n",
        "$$  SoftMax_{virginica}(Output Values) =  \\frac{e^{virginica}}{e^{setosa} * e^{versicolor} * e^{virginica} } $$\n",
        "\n",
        "$$  SoftMax_{virginica}(Output Values) =  \\frac{e^{0.23}}{e^{1.43} * e^{-0.4} * e^{0.23} } = 0.21 $$\n",
        "\n",
        "0.10 Softmax output value for versicolor.\n",
        "\n",
        "|Label|Raw output| SoftMax value |\n",
        "|---|---|---|\n",
        "|Setosa|1.43|0.69|\n",
        "|Versicolor|-0.4|0.10|\n",
        "|Virginica|0.23|0.21|\n",
        "\n",
        "\n",
        "- You can observe that largest value 1.43 is paired with largest softmax output value 0.69.\n",
        "- like wise for second largest value and last value.\n",
        "- We can see that the SoftMax function preserves the original order or ranking of Raw output values.\n",
        "- It is worth taking note that SoftMax gives output in range between 0 and 1.\n",
        "- that is something that SoftMax assures.\n",
        "- regardless of how many Row output values there are, SoftMax o/p values will always be between 0 and 1.\n",
        "\n",
        "## Probabilities and SoftMax\n",
        "\n",
        "0.69 + 0.10 + 0.21 = 1\n",
        "If we sum all the o/p values of SoftMax function that will be = 1\n",
        "That means that as long as the output are mutually exclusive then the SM o/p values can be interpreted as predicted \"Probabilities\".\n",
        "\n",
        "But Keep in mind that we can't trust its **accuracy** because they are dependent on weights and biases in the NNs.\n",
        "\n",
        "And weights and biases are in turn, dependent on rendomly selected initial values.\n",
        "\n",
        "so if we select the different initial value of parameters then, after training we endup getting optimal values that are as good as previously trained parameters to classify the data.\n",
        "\n",
        "But it will give different raw output values and so do the final softmax o/p values.\n",
        "\n",
        "**Takeaway**\n",
        "\n",
        "That means the predicted values are not only dependent on the different input values but also to the random initial values for weights and biases. so we can't put lot of trust on its probability.\n",
        "\n",
        "\n",
        "continue...\n",
        "\n",
        "\n",
        "## SoftMax Equation\n",
        "\n",
        "This is the general equation for the SoftMax function\n",
        "\n",
        "$$  SoftMax_{i}(Output Values) =  \\frac{e^{Output\\;value_i}}{ \\sum_{j=1}^ke^{Output \\; value_j}}$$\n",
        "\n",
        "i = individual raw o/p value\n",
        "\n",
        "eg: if i = 2, we are talking about raw o/p value for Versicolor.\n",
        "\n",
        "##  Takeaway\n",
        "\n",
        "ArgMax is easy to interpret but cannot be used in backpropagation, because its derivative is 0.\n",
        "\n",
        "In contrast SoftMax can be used in backpropagation as it has derivative"
      ],
      "metadata": {
        "id": "90Q7kC9p87Mf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "1lHPxk7dM0OX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "c5HTfBRD8zl5"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from math import e"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(values,i):\n",
        "  j = len(values)\n",
        "  sum = 0\n",
        "  for value in values:\n",
        "    sum += e**value\n",
        "  out = e**values[i]/sum\n",
        "  return out"
      ],
      "metadata": {
        "id": "nMyMBVp129FI"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_op = [1.43, -0.4, 0.23]\n",
        "softmax(raw_op, 0) # getting softmax value for setosa 1st value"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3nYY_dme4zSJ",
        "outputId": "e0685422-c899-4ddd-fe49-c960e673f9ba"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6841780769762262"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "softmax(raw_op, 1) # getting softmax value for Versicolor 2nd value\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xPfd3geW5J7P",
        "outputId": "c129eea4-bc39-48c0-bab5-b33a707b8fd8"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.10975144632131326"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "softmax(raw_op, 2) # getting softmax value for setosa 3rd value\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_fgzk2xm5TC-",
        "outputId": "d97c283c-5ecb-445a-f0be-a5fbc1d68093"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.20607047670246045"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Derivative of setosa\n",
        "# bump up its value by small amout of h\n",
        "# equation of derivative\n",
        "# d = (f(x+h) - f(x)) / h\n",
        "\n",
        "h = 0.0001\n",
        "\n",
        "# d of pred prob of setosa wrt raw o/p value of setosa\n",
        "new_raw_op = [1.43+h, -0.4, 0.23]\n",
        "swrts = (softmax(new_raw_op, 0) - softmax(raw_op, 0)) / h\n",
        "\n",
        "new_raw_op = [1.43, -0.4+h, 0.23] # bump up value of versicolor with small value of h\n",
        "s_wrt_vc = (softmax(new_raw_op, 0) - softmax(raw_op, 0)) / h\n",
        "\n",
        "new_raw_op = [1.43, -0.4, 0.23+h] # bump up value of virginica with small value of h\n",
        "s_wrt_virg = (softmax(new_raw_op, 0) - softmax(raw_op, 0)) / h\n",
        "\n",
        "print(f\"der of pred value of Setosa wrt raw o/p value of Setosa is : {swrts}\")\n",
        "print(f\"der of pred value of Versicolor wrt raw o/p value of Versicolor is : {s_wrt_vc}\")\n",
        "print(f\"der of pred value of Virginica wrt raw o/p value of Virginica is : {s_wrt_virg}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w7V7nDLN5Wya",
        "outputId": "563f7c4c-57fb-4e2d-eebb-126244072a08"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "der of pred value of Setosa wrt raw o/p value of Setosa is : 0.21607445616411702\n",
            "der of pred value of Versicolor wrt raw o/p value of Versicolor is : -0.07509246389925117\n",
            "der of pred value of Virginica wrt raw o/p value of Virginica is : -0.1409930465556819\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "So from above experiment we can see that the derivative of SoftMax is not always 0 and that's why we can use it for Gradient decent. rather than using the ArgMax which doen't have derivative or 0.\n",
        "\n",
        "So, NNs with multiple output often use SoftMax for training and then use ArgMax, which has easy to understand output, to classify new observation."
      ],
      "metadata": {
        "id": "55R-wfybI6TW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Next Cross entropy\n",
        "\n",
        "In example of working of neural network, I used sum of the squred residual loss function to determine how well the NN fit the data.\n",
        "\n",
        "However, when we use SoftMax function, and because the o/p values are predicted probabilities between 0 and 1, we often use **Cross entropy** to determine how well NN fits the data."
      ],
      "metadata": {
        "id": "jLPnEkufLsbO"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s-oBlV9t6Hnd",
        "outputId": "4b2a416f-15b4-4c17-fb2b-2f2828ff60e0"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-0.022617197629470898"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZMOmXYyKGMgA",
        "outputId": "59dd28c3-83d3-4e4d-f646-9cfaed915ba7"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.16361024418598547"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TWMt_5K3GhB4"
      },
      "execution_count": 20,
      "outputs": []
    }
  ]
}