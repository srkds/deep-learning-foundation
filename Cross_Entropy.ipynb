{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Cross Entropy\n",
        "\n",
        "## Prerequisite\n",
        "- Working of the neural nets, Backpropagation, and ArgMax and SoftMax.\n",
        "\n",
        "## Previous Work\n",
        "\n",
        "- In 1st example of working of neural networks it gives a simple output of any value. And to determine how well neural net fits the data we commanly use  the **Sum of the squred residuals**\n",
        "\n",
        "\n",
        "## Notes\n",
        "- But when we have neural networks with multiple output values just like the iris flower classifier NN I build previously.\n",
        "- there we often use ArgMax to easly interpret the results. Argmax doesnt have derivative or gives 0, given that we can;t use it for backpropagation.\n",
        "- So, in order to train neural networks we use SoftMax function.\n",
        "- And SoftMax output values are predicted probabilities between 0 and 1.\n",
        "- And when o/p is restricted between 0 and 1, we often use something called **Cross Entropy** to determine how well neural network fits the data.\n",
        "\n",
        "- Cross Entropy might sound fancy and complicated but when it comes to neural networks, is super simpel.\n",
        "\n",
        "Let's see by simple example.\n",
        "\n",
        "|Petal width|Sepal width|Species (observed/actual Iris Species)|\n",
        "|---|---|---|\n",
        "|0.04|0.42|Setosa|\n",
        "|1|0.52|Virginica|\n",
        "|0.50|0.37|Versicolor|\n",
        "\n",
        "**Step 1:** Get output values from previously trained Neural Network for all the inputs given in table.\n",
        "\n",
        "So, for 1st sample which has Petal width = 0.04 and Sepal width = 0.42 we get these o/p values Setosa = 0.57, Versicolor = 0.20, Virginica = 0.14\n",
        "\n",
        "We already know that data are from Setosa, the CrossEntropy is -log(SoftMax o/p value for Setosa) base `e`.\n",
        "\n",
        "\n",
        "$$ Cross Entropy_{observedClass} = -log_e(softmax \\; output \\; value \\; of \\;observed\\;class) $$\n",
        "\n",
        "$$ Cross Entropy_{setosa} = -log_e(0.57) $$\n",
        "\n",
        "In simple word, plug the predicted probability for observed species into Cross Entropy function.\n",
        "\n",
        "Above given Cross Entropy equation is the simplified form of this general equation.\n",
        "\n",
        "$$  Cross \\; Entropy_{class} =  -\\sum_{c=1}^{M}Observed_c * log(Predicted_c)$$\n",
        "\n",
        "As the current sample is of Setosa so observed value of it will be 1 and other two will become 0\n",
        "\n",
        "$$  = - (1) * log(Predicted_{setosa}) + (- (0) * log(Predicted_{versicolor})) + (- (0) * log(Predicted_{verginica}))$$\n",
        "\n",
        "multiply with 0 will become 0 so we will left with the simplified version of the equation that is described earlier.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7jizQM1XSADo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "PoAiu7H5Ry0e"
      },
      "outputs": [],
      "source": [
        "from math import log, e"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below is the table with SoftMax o/p values\n",
        "\n",
        "|Petal width|Sepal width|Species (observed/actual Iris Species)| SoftMax o/p value\n",
        "|---|---|---|---|\n",
        "|0.04|0.42|Setosa| 0.57|\n",
        "|1|0.52|Virginica| 0.58|\n",
        "|0.50|0.37|Versicolor| 0.52|"
      ],
      "metadata": {
        "id": "SyxgF0J-TKE9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ce_setosa = -log(0.57,e) # here 0.57 is Softmax o/p value\n",
        "ce_virginica = -log(0.58,e)\n",
        "ce_versicolor = -log(0.52, e)\n",
        "\n",
        "print(f\"Setosa Cross Entropy value: {ce_setosa}\")\n",
        "print(f\"Virginica Cross Entropy value: {ce_virginica}\")\n",
        "print(f\"Versicolor Cross Entropy value: {ce_versicolor}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aEzxLHJSShIK",
        "outputId": "50da2c1a-70e8-4bcf-aea6-1194b8950ab1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setosa Cross Entropy value: 0.5621189181535413\n",
            "Virginica Cross Entropy value: 0.5447271754416722\n",
            "Versicolor Cross Entropy value: 0.6539264674066639\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ce_setosa + ce_virginica + ce_versicolor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Np5MZ3xUkkF",
        "outputId": "93d76594-2bd5-4264-9858-9c993aed2ec7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.7607725610018772"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally we get cross entropy value for each sample\n",
        "\n",
        "|Petal width|Sepal width|Species (observed/actual Iris Species)| SoftMax o/p value | Cross Entropy value\n",
        "|---|---|---|---|---|\n",
        "|0.04|0.42|Setosa| 0.57|  0.562118918153541\n",
        "|1|0.52|Virginica| 0.58| 0.5447271754416722\n",
        "|0.50|0.37|Versicolor| 0.52| 0.6539264674066639\n",
        "\n",
        "Now add up all the cross entropy values, we get Total Cross Entropy = 1.7607725610018772\n",
        "\n",
        "- based on Total Cross Entropy value we can use Backpropagation to adjust Weights and Biases and minimize the total error."
      ],
      "metadata": {
        "id": "z8SVr7qyUKnd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Why Cross Entropy and not something like Squred Residuals as a loss function?\n",
        "\n",
        "- If we can calculate probabilities for each observed species then we can calculate residuals\n",
        "\n",
        "- Residual = Observed - predicted\n",
        "- The difference between observed probabilities and predicted probabilities\n",
        "- eg: 1st row is of observed species Setosa, and thus observed probability is 1 and predicted prob is 0.57. thus Residual = 0.43\n",
        "\n",
        "residual = 1 - 0.57 = 0.43\n",
        "\n",
        "residual^2 = (0.43)^2 = 0.18\n",
        "\n",
        "and we can now calculate sum of the squred residuals\n",
        "\n",
        "**Worth taking notes**\n",
        "\n",
        "- Remember, SoftMax only gives us values between 0 and 1,\n",
        "  - so if prediction is for setosa is really good,\n",
        "  - it will be close to 0, and if prediction is worst it will be close to 0,\n",
        "  - In above example prediction for Setosa is kind of in the middle 0.57\n",
        "- we can plug values for \"p\" from 0 to 1 into cross entropy function and plot the o/p in function CE = -log(\"p\").\n",
        "- Y axis is the loss, shows how bad the prediction is.\n",
        "- When we use Cross Entropy, as the prediction gets worse and worse meaning closer to 0,the Loss kind of explodes and gets really big.\n",
        "- In contrast, if we plug values for \"p\" from 0 to 1 into SSR. the change in Loss is not as large as it is for Cross Entropy.\n",
        "- As step size for Backpropagation depends in part, on derivatives of these functions.\n",
        "- And derivative, or slope of the tangent line for Cross Entropy for bad prediction will be relatively large compared to derivative for that same, bad prediction with SSR.\n",
        "- So when NN makes Bad prediction, Cross Entropy will help us to take relatively large steps towards a better prediction. because slope of the tangent line will be relatively large.\n",
        "- see below image to see difference.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/srkds/deep-learning-foundation/main/assets/CE_slope.jpg\" width=\"50%\"/>\n",
        "\n",
        "## Takeaway\n",
        "\n",
        "- Based on above tradeoff, select loss function that gives large derivative or slope when prediction is bad. so that it can help take big steps to move towards good prediction.\n"
      ],
      "metadata": {
        "id": "FsOsjjXkZq1o"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cVoQllJESu-d"
      },
      "execution_count": 5,
      "outputs": []
    }
  ]
}